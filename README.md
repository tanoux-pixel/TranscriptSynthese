# TranscripteurSynth√®se IA - Sant√© / Qualit√© (Version Mistral)

## Description

**TranscripteurSynth√®se** est un script Python tout-en-un qui permet :

- üé§ La transcription automatique de fichiers audio, vid√©o ou de liens YouTube (via Whisper)
- üìù La g√©n√©ration d'une synth√®se IA avanc√©e : choix du public cible, contexte, niveau de langue, style, extraction de bullet points, citations, glossaire, recommandations, encadr√© juridique, etc.
- üìÑ L'export direct en **Word (.docx)**
- üñ•Ô∏è Une utilisation aussi bien en terminal (menu interactif) qu'en **API** (FastAPI)
- ü§ñ Utilisation de **Mistral local** via Transformers (mod√®les : Mistral-7B-Instruct, etc.), avec optimisations m√©moire et GPU

---

## Fonctionnalit√©s principales

- D√©tection automatique du type de fichier (audio, vid√©o, texte, YouTube)
- Transcription multilingue avec **Whisper**
- Menus interactifs pour personnaliser la synth√®se (public, contexte, style‚Ä¶)
- G√©n√©ration d'un prompt IA ultra-adapt√© √† vos besoins
- Synth√®se compl√®te : r√©sum√©, structuration, analyse, recommandations, glossaire, citations, analyse critique, encadr√© juridique
- Export Word pr√™t √† l'usage professionnel
- Double usage : terminal **et** API (mode batch ou int√©gr√© √† un autre outil)
- **Nouveau** : IA locale avec Mistral via Transformers, sans d√©pendance externe

---

## Nouveaut√©s Version 2.1 (Mistral)

### üÜï Int√©gration Mistral Local
- **Remplacement d'Ollama** par Mistral directement via Transformers
- **Cache intelligent** : le mod√®le reste en m√©moire entre les appels
- **Optimisations m√©moire** : quantification 8-bit automatique si disponible
- **Support GPU/CPU** : d√©tection automatique et utilisation optimale

### üöÄ Am√©lirations performances
- **Choix de mod√®les** : Mistral-7B-Instruct-v0.1/v0.2 selon vos besoins
- **Gestion m√©moire** : optimisations pour fonctionner sur machines 8-16GB RAM
- **Templates de chat** : utilisation du format officiel Mistral
- **Contr√¥le de g√©n√©ration** : repetition penalty et param√®tres fins

---

## Installation

### 1. **Cloner le d√©p√¥t**
   ```bash
   git clone <lien-du-repo>
   cd <nom-du-repo>
   ```

### 2. **Cr√©er et activer un environnement virtuel** (recommand√©)
   ```bash
   python3 -m venv mistral_env
   source mistral_env/bin/activate  # Linux/Mac
   # ou
   mistral_env\Scripts\activate     # Windows
   ```

### 3. **Installer les d√©pendances**
   ```bash
   pip install torch whisper yt-dlp requests python-docx fastapi uvicorn transformers accelerate
   ```

### 4. **Installation GPU (optionnel, recommand√©)**
   ```bash
   # Pour NVIDIA GPU avec CUDA 11.8
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   
   # Pour optimisations m√©moire (optionnel)
   pip install bitsandbytes
   ```

### 5. **Premier lancement (t√©l√©chargement du mod√®le)**
   ```bash
   python transcriptsynthese.py
   # Le mod√®le Mistral sera t√©l√©charg√© automatiquement au premier usage (~13GB)
   ```

---

## Configuration requise

### Minimum
- **Python 3.8+**
- **8 GB RAM** (pour Mistral-7B)
- **15 GB espace disque** (mod√®les + cache)
- **CPU moderne** (Intel i5/AMD Ryzen 5 ou sup√©rieur)

### Recommand√©
- **16 GB RAM** ou plus
- **GPU NVIDIA** avec 6GB+ VRAM (RTX 3060, RTX 4060, etc.)
- **SSD** pour stockage des mod√®les

---

## Utilisation

### Mode Terminal (Interactif)

1. **Lancer le script**
   ```bash
   python transcriptsynthese.py
   ```

2. **Suivre les menus :**
   - Indiquer le fichier (audio, vid√©o, texte) ou URL YouTube √† traiter
   - Choisir le niveau de transcription Whisper (tiny, base, small‚Ä¶)
   - Choisir les param√®tres de synth√®se IA (public, contexte, style, structuration, etc.)
   - S√©lectionner le mod√®le Mistral (7B-Instruct-v0.1 par d√©faut)
   - Lancer la g√©n√©ration de la synth√®se IA
   - R√©cup√©rer le fichier Word (.docx) g√©n√©r√©

### Mode API

1. **Lancer l'API**
   ```bash
   python transcriptsynthese.py --api
   ```

2. **Utiliser l'endpoint**
   ```bash
   curl -X POST "http://localhost:8000/synthese/" \
        -F "file=@mon_texte.txt" \
        -F "public=Professionnels de sant√©" \
        -F "modele=mistralai/Mistral-7B-Instruct-v0.1"
   ```

3. **Documentation interactive**
   - Acc√©der √† `http://localhost:8000/docs` pour l'interface Swagger

---

## Mod√®les Mistral disponibles

### Par d√©faut inclus

| Mod√®le | Taille | RAM requise | Usage recommand√© |
|--------|--------|-------------|------------------|
| `mistralai/Mistral-7B-Instruct-v0.1` | 7B | 8-12 GB | Production, √©quilibre qualit√©/performance |
| `mistralai/Mistral-7B-Instruct-v0.2` | 7B | 8-12 GB | Version am√©lior√©e, instructions plus fines |
| `mistralai/Mistral-7B-v0.1` | 7B | 8-12 GB | Version de base, plus rapide |

### Optimisations automatiques
- **Quantification 8-bit** : r√©duction RAM √† ~6GB si bitsandbytes install√©
- **Device mapping** : r√©partition automatique GPU/CPU selon disponibilit√©
- **Cache persistant** : le mod√®le reste charg√© entre les synth√®ses

---

## Exemples d'usage

### Transcription + Synth√®se compl√®te
```bash
# Fichier audio vers synth√®se Word
python transcriptsynthese.py
> Chemin: meeting_qualite.mp3
> Mod√®le Whisper: 3 (small)
> Public: Professionnels de sant√©
> Contexte: Rapport interne/qualit√©
# ‚Üí meeting_qualite_transcription.txt + synthese_finale.docx
```

### YouTube vers rapport
```bash
python transcriptsynthese.py
> Chemin: https://youtube.com/watch?v=xyz123
> Public: D√©cideurs/gestionnaires
> Style: Pr√™t √† publier
# ‚Üí T√©l√©chargement auto + transcription + synth√®se structur√©e
```

### API pour int√©gration
```python
import requests

with open("texte.txt", "rb") as f:
    response = requests.post(
        "http://localhost:8000/synthese/",
        files={"file": f},
        data={
            "public": "Grand public",
            "contexte": "Communication √† destination des usagers",
            "modele": "mistralai/Mistral-7B-Instruct-v0.2"
        }
    )
# ‚Üí Fichier Word en r√©ponse
```

---

## D√©pannage

### Erreur "CUDA out of memory"
```bash
# Utiliser la quantification 8-bit
pip install bitsandbytes
# Ou forcer CPU
export CUDA_VISIBLE_DEVICES=""
```

### Mod√®le trop lent
```bash
# Utiliser un mod√®le plus petit ou optimiser
# Dans le script, modifier load_mistral_model() :
model_kwargs["load_in_4bit"] = True  # Quantification aggressive
```

### Erreur de t√©l√©chargement du mod√®le
```bash
# V√©rifier la connexion et l'espace disque
df -h  # Linux/Mac
dir   # Windows
# Cache Hugging Face : ~/.cache/huggingface/
```

### API ne d√©marre pas
```bash
# V√©rifier les d√©pendances FastAPI
pip install fastapi uvicorn --upgrade
```

---

## Architecture technique

```
TranscripteurSynth√®se/
‚îú‚îÄ‚îÄ transcriptsynthese.py    # Script principal avec Mistral
‚îú‚îÄ‚îÄ README.md               # Cette documentation
‚îú‚îÄ‚îÄ exigence.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ changes.log            # Historique des versions
‚îî‚îÄ‚îÄ cache/                 # Cache des mod√®les (auto-cr√©√©)
    ‚îú‚îÄ‚îÄ whisper/           # Mod√®les Whisper
    ‚îî‚îÄ‚îÄ huggingface/       # Mod√®les Mistral
```

### Flux de traitement
1. **Input** ‚Üí D√©tection format (audio/vid√©o/texte/YouTube)
2. **Transcription** ‚Üí Whisper (si n√©cessaire)
3. **Param√©trage** ‚Üí Menus interactifs
4. **Prompt** ‚Üí Construction adapt√©e au contexte
5. **IA** ‚Üí Mistral local via Transformers
6. **Export** ‚Üí Document Word structur√©

---

## Performance et benchmarks

### Temps de traitement typiques (Mistral-7B, RTX 4060)
- **Transcription 10min audio** : ~2-3 minutes (Whisper small)
- **Synth√®se 5000 mots** : ~30-60 secondes
- **Export Word** : instantan√©

### Consommation m√©moire
- **CPU seul** : 12-16 GB RAM
- **GPU + quantification** : 6-8 GB RAM + 6 GB VRAM
- **Cache disque** : ~13 GB (mod√®le) + transcriptions

---

## D√©pendances

### Essentielles
```
torch>=2.0.0
whisper>=20231117
yt-dlp>=2023.12.30
requests>=2.31.0
python-docx>=1.1.0
transformers>=4.36.0
accelerate>=0.25.0
```

### API (optionnel)
```
fastapi>=0.104.0
uvicorn>=0.24.0
```

### Optimisations (optionnel)
```
bitsandbytes>=0.41.0    # Quantification 8-bit
torch-audio             # Support audio √©tendu
```

---

## S√©curit√© et confidentialit√©

### ‚úÖ Avantages Mistral local
- **Donn√©es priv√©es** : aucun envoi vers le cloud
- **Offline** : fonctionne sans internet (apr√®s t√©l√©chargement)
- **Contr√¥le total** : param√®tres, mod√®les, cache local
- **RGPD compliant** : traitement local des donn√©es sensibles

### üîí Bonnes pratiques
- **Environnement virtuel** : isolation des d√©pendances
- **Cache s√©curis√©** : v√©rifier les permissions du dossier ~/.cache/
- **Nettoyage** : supprimer les fichiers temporaires apr√®s usage

---

## Migration depuis Ollama

Si vous utilisez la version pr√©c√©dente avec Ollama :

1. **Sauvegarder** vos param√®tres et scripts existants
2. **Installer** les nouvelles d√©pendances : `pip install transformers accelerate`
3. **Remplacer** le fichier `transcriptsynthese.py`
4. **Premier lancement** : le mod√®le Mistral se t√©l√©charge automatiquement
5. **D√©sinstaller Ollama** si souhait√© (optionnel)

### Diff√©rences principales
- **Plus d'installation Ollama** requise
- **Interface identique** : menus et API inchang√©s
- **Performance am√©lior√©e** : optimisations Transformers natives
- **Cache unifi√©** : avec autres mod√®les Hugging Face

---

## Contribution et support

### Auteur
Projet d√©velopp√© par **Tanoux_Pixel**

### Licence
**CC-BY-NC** (Creative Commons Attribution - Pas d'utilisation commerciale)
Voir : https://fr.wikipedia.org/wiki/Licence_Creative_Commons

### Support
- **Issues** : Signaler les bugs via GitHub Issues
- **Am√©liorations** : Pull requests bienvenues
- **Documentation** : Contributions √† la doc appr√©ci√©es

### Roadmap
- [ ] Support des mod√®les Mistral plus grands (22B+)
- [ ] Interface web (Streamlit/Gradio)
- [ ] Export formats multiples (PDF, HTML, Markdown)
- [ ] Batch processing automatis√©
- [ ] Int√©gration bases de donn√©es

---

## Exemples concrets secteur sant√©

### Cas d'usage typiques

#### 1. Transcription r√©union CVS
```
Input: reunion_cvs_2024.mp3
Public: Institutionnels (ARS, HAS‚Ä¶)
Contexte: Dossier de certification (HAS‚Ä¶)
‚Üí Synth√®se avec r√©f√©rences r√©glementaires automatiques
```

#### 2. Formation personnel soignant
```
Input: formation_hygiene.mp4
Public: Professionnels de sant√©
Style: P√©dagogique (explications, d√©finitions‚Ä¶)
‚Üí Support avec glossaire et recommandations pratiques
```

#### 3. Communication patients
```
Input: https://youtube.com/watch?v=info-diabete
Public: Patients/r√©sidents
Style: Tr√®s grand public (aucun jargon)
‚Üí Fiche d'information accessible et structur√©e
```

---

*Version 2.1 - Mistral Local | Derni√®re mise √† jour : 2025*
